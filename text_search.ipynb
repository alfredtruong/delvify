{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba59712",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ipynb_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run class_def.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe30f19",
   "metadata": {},
   "source": [
    "# IMPLEMENT TEXT SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe18045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49f7f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d=Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.raw.head(1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b69ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a59ffe",
   "metadata": {},
   "source": [
    "### build up regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a20c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d.df['title'].str.lower().str.contains('summer|hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa0c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.df['title'].str.contains('womens|me|dress|banana',flags=re.IGNORECASE,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fae82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        d.df['title'],\n",
    "        d.df['title'].str.contains('sans',flags=re.IGNORECASE,regex=True)+0\n",
    "    ],\n",
    "    axis=1,\n",
    ")[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc5a35e",
   "metadata": {},
   "source": [
    "### combine description / tag columns to do single regex on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90177ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d.df[['title','title_orig','tags']].apply(lambda x:x['title']+x['title_orig']+x['tags'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68678f",
   "metadata": {},
   "source": [
    "# FUNCTIONS TO BUILD UP QUERY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84cd1a1",
   "metadata": {},
   "source": [
    "### search string tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d01d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEARCH_STRING = str\n",
    "SEARCH_TOKENS = List[str]\n",
    "def tokenize_search_string(\n",
    "    search_string : SEARCH_STRING,\n",
    "    exact_search  : bool = True,\n",
    "    ) -> SEARCH_TOKENS :\n",
    "    tokens = re.split('\\W+',search_string) # partition string on non-alphanumeric chars\n",
    "    tokens = [token.lower() for token in tokens] # set tokens to lower case\n",
    "    tokens = list(set(tokens)) # uniques\n",
    "    if exact_search: tokens = ['\\\\b'+x+'\\\\b' for x in tokens] # force exact match, dont want to match 'red' from 'altered'\n",
    "    return tokens\n",
    "\n",
    "#bool(re.search('\\\\bsets\\\\b',merged_product_identifier_text()[0],flags=re.IGNORECASE))\n",
    "\n",
    "display(tokenize_search_string('absc#dd\tddd  ,#,      asd;f    asdf asdf asdf asdf asdf asdfsd000_22220'))\n",
    "display(tokenize_search_string('womens banana    dress dress me me me'))\n",
    "display(tokenize_search_string('womens banana    dress dress me me me',exact_search=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c64a5",
   "metadata": {},
   "source": [
    "### build up regex pattern from search string tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de253b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tokens_to_re_pattern(search_tokens : SEARCH_TOKENS) -> str :\n",
    "    #return '\\b'+('\\b|\\b'.join(search_tokens))+'\\b'\n",
    "    return '|'.join(search_tokens)\n",
    "\n",
    "display(search_tokens_to_re_pattern(tokenize_search_string('womens banana    dress dress me me me')))\n",
    "display(search_tokens_to_re_pattern(tokenize_search_string('womens banana    dress dress me me me',exact_search=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db37b5",
   "metadata": {},
   "source": [
    "### simple way to get description + tags to regex over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60030fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.df[['title','title_orig','tags','product_color']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_product_identifier_text() -> pd.Series(dtype='str') :\n",
    "    descriptor_colnames = ['title','title_orig','tags','product_color'] # cols of interest\n",
    "    df_descriptors      = d.df[descriptor_colnames] # extract columns\n",
    "    df_descriptors      = df_descriptors.apply(lambda x:' '.join([str(x) for x in x.values]),axis=1) # merge with | as sep\n",
    "    #df_descriptors      = df_descriptors.str.replace('\\'','') # strip appostrophes\n",
    "    df_descriptors      = df_descriptors.str.replace('\\W',' ',regex=True) # strip special chars\n",
    "    return df_descriptors\n",
    "merged_product_identifier_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca9086",
   "metadata": {},
   "source": [
    "### count how many tokens found in each product description / tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b442bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# weighted token search\n",
    "# looking at all string product identifiers (title, titie_orig, tags)\n",
    "# return weighted value of tokens found (e.g. 3x 2x 1x)\n",
    "def tokens_found_count(\n",
    "    pd_series     : pd.Series(dtype='str'),\n",
    "    search_string : str,\n",
    "    top_n         : int  = 5,\n",
    "    exact_search  : bool = False,\n",
    "    verbose       : int  = 0, # show workings\n",
    "    ) -> pd.Series(dtype='int') :\n",
    "    ###########################################################\n",
    "    # identify which tokens matched\n",
    "    ###########################################################\n",
    "    # strip tokens from search string\n",
    "    search_tokens = tokenize_search_string(search_string=search_string,exact_search=exact_search)\n",
    "    \n",
    "    # figure out which matched / get List[pd.Series(dtype=bool)]\n",
    "    found_count_list = [\n",
    "        pd_series.apply(\n",
    "            lambda x:bool(re.search(token,x,flags=re.IGNORECASE))\n",
    "        ) for token in search_tokens\n",
    "    ]\n",
    "    \n",
    "    # df of Trues/Falses\n",
    "    found_count_df = pd.concat(found_count_list,axis=1)\n",
    "    found_count_df.columns = search_tokens\n",
    "    \n",
    "    # series of found cfound_count_series\n",
    "    found_count_series = found_count_df.sum(axis=1)\n",
    "    found_count_series.name = 'tokens_found_count'\n",
    "    \n",
    "    ###########################################################\n",
    "    # token rarity\n",
    "    ###########################################################\n",
    "    # count rarity of token - rare tokens should be valued more\n",
    "    token_found_count = found_count_df.sum()\n",
    "    token_rarity = 1 - token_found_count / len(found_count_df) # rare tokens are valued more, only 1 instance (value ~= 1), 50% of products (value = 50%), 100% of products (value ~= 0)\n",
    "    discounted_token_df = found_count_df * token_rarity # impact token found bool with value of token (between 0 and 1)\n",
    "    discounted_token_series = discounted_token_df.sum(axis=1)\n",
    "    discounted_token_df.columns = ['discounted('+x+')' for x in discounted_token_df.columns] # rename df columns so result can exist in same df\n",
    "    \n",
    "    ###########################################################\n",
    "    # give feedback if token not found\n",
    "    ###########################################################\n",
    "    if verbose>1: print('token_found_count');display(token_found_count)\n",
    "    unmatched_tokens = token_found_count[token_found_count==0]\n",
    "    \n",
    "    pad_string = lambda s : '\\'' + s + '\\''\n",
    "    \n",
    "    # only give feedback if there are useless tokens\n",
    "    if len(unmatched_tokens)>0:\n",
    "        useless_tokens = [pad_string(s) for s in unmatched_tokens.index]\n",
    "        if verbose>1: display(useless_tokens)\n",
    "        if len(useless_tokens)==1:\n",
    "            helpful_string = useless_tokens[0]\n",
    "        else:\n",
    "            helpful_string = ', '.join(useless_tokens[:-1]) + ' and ' + useless_tokens[-1]\n",
    "\n",
    "        # show helping string\n",
    "        print(f'searchbot: no matches for {helpful_string}, try looking for something else')\n",
    "        print()\n",
    "\n",
    "    ###########################################################\n",
    "    # make pretty return df\n",
    "    ###########################################################\n",
    "    # [ found count ] + [ which tokens found ] + [search string]\n",
    "    found_count_summary = pd.concat(\n",
    "        [\n",
    "            discounted_token_series.to_frame('discounted(tokens matched)'),\n",
    "            discounted_token_df,\n",
    "            found_count_series.to_frame('tokens matched'),\n",
    "            found_count_df,\n",
    "            pd_series.to_frame()\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # snip results table to stuff which has at least SOME match\n",
    "    found_count_summary = found_count_summary[found_count_summary['tokens matched']>0]\n",
    "    # sort on some metric\n",
    "    found_count_summary = found_count_summary.sort_values(['tokens matched']+search_tokens,ascending=False)\n",
    "\n",
    "    ###########################################################\n",
    "    # results chat\n",
    "    ###########################################################\n",
    "    if len(found_count_summary)==0:\n",
    "        print(f'results: I got nothing! T⌓T')\n",
    "    elif len(found_count_summary)==1:\n",
    "        print(f'results: only 1 hit ￣ω￣, I hope it\\'s what you wanted!')\n",
    "    elif len(found_count_summary)<10:\n",
    "        print(f'results: {len(found_count_summary)} items found, I can do better if you can be more specific')\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # tag on searched string as well for debug purposes\n",
    "    if verbose>0:\n",
    "        found_count_summary = found_count_summary.join(pd_series.to_frame())\n",
    "    ''' \n",
    "    \n",
    "    ###########################################################\n",
    "    # plot top_n results\n",
    "    ###########################################################\n",
    "    if len(found_count_summary)>0:\n",
    "        d.get_product_pictures(locs=found_count_summary.index[:top_n])\n",
    "        plt.show()\n",
    "\n",
    "    ###########################################################\n",
    "    # return\n",
    "    ###########################################################\n",
    "    if verbose>0:\n",
    "        return found_count_summary\n",
    "    else:\n",
    "        return found_count_summary.head(top_n)\n",
    "\n",
    "#tokens_found_count(merged_product_identifier_text(),['summer','short'],verbose=1).head(10) # basic logic\n",
    "#tokens_found_count(merged_product_identifier_text(),['harajuku','goth','sexy'],verbose=1).head(10) # test rare token logic\n",
    "#tokens_found_count(merged_product_identifier_text(),['kids','top','banana','bobby','henry'],verbose=0).head(10) # test bad search tokens\n",
    "#tokens_found_count(merged_product_identifier_text(),['kids','sandals'],verbose=0).head(10) # test\n",
    "#tokens_found_count(merged_product_identifier_text(),['balloon'],top_n=5,verbose=0) # test top_n\n",
    "#tokens_found_count(merged_product_identifier_text(),['balloon'],top_n=10,verbose=1).head(20) # test results snipping\n",
    "#tokens_found_count(merged_product_identifier_text(),['banana'],top_n=10,verbose=1).head(20) # test results chat\n",
    "#tokens_found_count(merged_product_identifier_text(),['sandal'],top_n=10,verbose=1).head(20) # test results chat\n",
    "#tokens_found_count(merged_product_identifier_text(),['sandal','red'],top_n=10,verbose=1).head(20) # test results chat\n",
    "#tokens_found_count(merged_product_identifier_text(),'sandal red',top_n=10,verbose=1).head(20) # test results chat\n",
    "#tokens_found_count(merged_product_identifier_text(),['top','skinny','red'],top_n=10,verbose=1).head(20) # test results chat\n",
    "#tokens_found_count(merged_product_identifier_text(),'top red',top_n=5,verbose=1).head(20) # test results chat\n",
    "#tokens_found_count(merged_product_identifier_text(),'top red',top_n=5,exact_search=True,verbose=1).head(20) # test exact search\n",
    "#tokens_found_count(merged_product_identifier_text(),'exy',top_n=5,verbose=1).head(20) # test exact search\n",
    "#tokens_found_count(merged_product_identifier_text(),'exy',top_n=5,exact_search=True,verbose=1).head(20) # test exact search\n",
    "tokens_found_count(merged_product_identifier_text(),'top red',top_n=5,exact_search=True,verbose=1).head(20) # test exact search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
