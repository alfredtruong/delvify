{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb5a0b05",
   "metadata": {},
   "source": [
    "# [TLDR] PROGRESSIVELY BUILT UP A TEXT SEARCH\n",
    "- main points include\n",
    "\n",
    "1. input `search_string` broken up (tokenized) on `\\W` special chars\n",
    "\n",
    "2. can look for tokens in \"product descriptor\" either by (a) exact matching tokens OR (b) simple matching of tokens\n",
    "- exact matching\n",
    "  - pros\n",
    "    - avoids matching the token `red` in strings like `prediction`, i.e. avoids spurious matches\n",
    "  - cons\n",
    "    - tags are not all space separated so will not match anything, i.e. cannot find `white` in `whitesandalsbeach` \n",
    "- simple matching\n",
    "  - pros and cons are literally flip of that in exact matching, hard to say which is better\n",
    "  \n",
    "3. \"product descriptor\" is a merge of `title`, `title_orig`, `tags` and `product_color`\n",
    "- all seem useful\n",
    "- `title` and `title_orig` should be similar, one is French the other is English translation, who knows which is better, i.e. use both\n",
    "- `tags` seem very useful but format isn't standardized, some are space separated, others `;` separated, others NOT separated\n",
    "- `product_color` marginally useful\n",
    "\n",
    "4. given n tokens extracted from the `search_string`, trying to find matches for each token\n",
    "- can rank products on the number of `n` token hits\n",
    "- can also do a weighted token hit (based on the rarity of a token), i.e. rare tokens are stronger hits than common tokens, \"goth\" vs \"summer\"\n",
    "\n",
    "5. search tokens that receive 0 hits on the product list are useless, can give hints to the user to try different search strings\n",
    "\n",
    "6. top n text hits can be return along with their pictures as well for debugging / logic checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba59712",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ipynb_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run class_def.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe30f19",
   "metadata": {},
   "source": [
    "# IMPLEMENT TEXT SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac49f7f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d=Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2563eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.raw.head(1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b69ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a59ffe",
   "metadata": {},
   "source": [
    "### build up regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a20c6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d.df['title'].str.lower().str.contains('summer|hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa0c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.df['title'].str.contains('womens|me|dress|banana',flags=re.IGNORECASE,regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fae82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    [\n",
    "        d.df['title'],\n",
    "        d.df['title'].str.contains('sans',flags=re.IGNORECASE,regex=True)+0\n",
    "    ],\n",
    "    axis=1,\n",
    ")[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc5a35e",
   "metadata": {},
   "source": [
    "### combine description / tag columns to do single regex on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90177ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d.df[['title','title_orig','tags']].apply(lambda x:x['title']+x['title_orig']+x['tags'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68678f",
   "metadata": {},
   "source": [
    "# FUNCTIONS TO BUILD UP QUERY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84cd1a1",
   "metadata": {},
   "source": [
    "### search string tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d01d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEARCH_STRING = str\n",
    "SEARCH_TOKENS = List[str]\n",
    "def tokenize_search_string(\n",
    "    search_string : SEARCH_STRING,\n",
    "    ) -> SEARCH_TOKENS :\n",
    "    tokens = re.split('\\W+',search_string) # partition string on non-alphanumeric chars\n",
    "    tokens = [token.lower() for token in tokens] # set tokens to lower case\n",
    "    tokens = list(set(tokens)) # uniques\n",
    "    return tokens\n",
    "\n",
    "def exactify_search_tokens(\n",
    "    search_tokens : SEARCH_TOKENS,\n",
    "    ) -> SEARCH_TOKENS :\n",
    "    return ['\\\\b'+token+'\\\\b' for token in search_tokens] # force exact match, dont want to match 'red' from 'altered'\n",
    "\n",
    "#bool(re.search('\\\\bsets\\\\b',make_product_descriptor()[0],flags=re.IGNORECASE))\n",
    "\n",
    "display(tokenize_search_string('absc#dd\tddd  ,#,      asd;f    asdf asdf asdf asdf asdf asdfsd000_22220'))\n",
    "display(tokenize_search_string('womens banana    dress dress me me me'))\n",
    "display(exactify_search_tokens(tokenize_search_string('womens banana    dress dress me me me')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224c64a5",
   "metadata": {},
   "source": [
    "### build up regex pattern from search string tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de253b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tokens_to_re_pattern(search_tokens : SEARCH_TOKENS) -> str :\n",
    "    #return '\\b'+('\\b|\\b'.join(search_tokens))+'\\b'\n",
    "    return '|'.join(search_tokens)\n",
    "\n",
    "display(search_tokens_to_re_pattern(tokenize_search_string('womens banana    dress dress me me me')))\n",
    "display(search_tokens_to_re_pattern(exactify_search_tokens(tokenize_search_string('womens banana    dress dress me me me'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db37b5",
   "metadata": {},
   "source": [
    "### simple way to get description + tags to regex over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60030fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.df[['title','title_orig','tags','product_color']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLNAME = str\n",
    "\n",
    "def make_product_descriptor(\n",
    "    descriptor_colnames : List[COLNAME] = ['title','title_orig','tags','product_color'] # cols of interest\n",
    "    ) -> pd.Series(dtype='str') :\n",
    "    descriptors = d.df[descriptor_colnames] # extract columns\n",
    "    descriptors = descriptors.apply(lambda x:' '.join([str(x) for x in x.values]),axis=1) # merge with | as sep\n",
    "    #descriptors = descriptors.str.replace('\\'','') # strip appostrophes\n",
    "    descriptors = descriptors.str.replace('\\W',' ',regex=True) # strip special chars\n",
    "    return descriptors\n",
    "make_product_descriptor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aca9086",
   "metadata": {},
   "source": [
    "### count how many tokens found in each product description / tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b442bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SEARCH_STRING = str\n",
    "TOKENS        = str\n",
    "\n",
    "# weighted token search\n",
    "# looking at all string product identifiers (title, titie_orig, tags)\n",
    "# return weighted value of tokens found (e.g. 3x 2x 1x)\n",
    "def tokens_found_count(\n",
    "    product_descriptor : pd.Series(dtype='str'),\n",
    "    search_tokens      : List[TOKENS]  = None,\n",
    "    search_string      : SEARCH_STRING = None,\n",
    "    exact_search       : bool          = False,\n",
    "    show_top_n         : int           = 5,\n",
    "    verbose            : int           = 0, # show workings\n",
    "    ) -> pd.Series(dtype='int') :\n",
    "    ###########################################################\n",
    "    # get search tokens\n",
    "    ###########################################################\n",
    "    if search_tokens is None:\n",
    "        search_tokens = tokenize_search_string(search_string=search_string) # strip them from search_string if search_tokens NOT provided\n",
    "\n",
    "    ###########################################################\n",
    "    # amend tokens to exactify search on request\n",
    "    ###########################################################\n",
    "    if exact_search:\n",
    "        search_tokens_orig = search_tokens\n",
    "        search_tokens = exactify_search_tokens(search_tokens)\n",
    "        \n",
    "    ###########################################################\n",
    "    # figure out which tokens are found in product_descriptor\n",
    "    ###########################################################\n",
    "    # returns a List[pd.Series(dtype=bool)], each list entry denoteing if nth token is found (exact or simple matching)\n",
    "    found_count_list = [\n",
    "        product_descriptor.apply(\n",
    "            lambda x:bool(re.search(token,x,flags=re.IGNORECASE))\n",
    "        ) for token in search_tokens\n",
    "    ]\n",
    "    \n",
    "    # df of Trues/Falses\n",
    "    found_count_df = pd.concat(found_count_list,axis=1)\n",
    "    found_count_df.columns = search_tokens\n",
    "    \n",
    "    # series of found_count_series\n",
    "    found_count_series = found_count_df.sum(axis=1)\n",
    "    found_count_series.name = 'tokens_found_count'\n",
    "    \n",
    "    ###########################################################\n",
    "    # token rarity\n",
    "    ###########################################################\n",
    "    # count rarity of token - rare tokens should be valued more\n",
    "    token_found_count = found_count_df.sum()\n",
    "    token_rarity = 1 - token_found_count / len(found_count_df) # rare tokens are valued more, only 1 instance (value ~= 1), 50% of products (value = 50%), 100% of products (value ~= 0)\n",
    "    discounted_token_df = found_count_df * token_rarity # impact token found bool with value of token (between 0 and 1)\n",
    "    discounted_token_series = discounted_token_df.sum(axis=1)\n",
    "    discounted_token_df.columns = ['discounted('+x+')' for x in discounted_token_df.columns] # rename df columns so result can exist in same df\n",
    "    \n",
    "    ###########################################################\n",
    "    # give feedback if token not found\n",
    "    ###########################################################\n",
    "    if verbose>1: print('token_found_count');display(token_found_count)\n",
    "    unmatched_tokens = token_found_count[token_found_count==0]\n",
    "    \n",
    "    pad_string = lambda s : '\\'' + s + '\\''\n",
    "    \n",
    "    # only give feedback if there are useless tokens\n",
    "    if len(unmatched_tokens)>0:\n",
    "        useless_tokens = [pad_string(s) for s in unmatched_tokens.index]\n",
    "        if verbose>1: display(useless_tokens)\n",
    "        if len(useless_tokens)==1:\n",
    "            helpful_string = useless_tokens[0]\n",
    "        else:\n",
    "            helpful_string = ', '.join(useless_tokens[:-1]) + ' and ' + useless_tokens[-1]\n",
    "\n",
    "        # show helping string\n",
    "        if exact_search:\n",
    "            helpful_string = helpful_string.replace('\\\\b','')\n",
    "            print(f'searchbot: no exact matches for {helpful_string}, try searching for something else')\n",
    "        else:\n",
    "            print(f'searchbot: no matches for {helpful_string}, try searching for something else')\n",
    "        print()\n",
    "\n",
    "    ###########################################################\n",
    "    # make pretty return df\n",
    "    ###########################################################\n",
    "    # [ found count ] + [ which tokens found ] + [search string]\n",
    "    found_count_summary = pd.concat(\n",
    "        [\n",
    "            discounted_token_series.to_frame('discounted(tokens matched)'),\n",
    "            discounted_token_df,\n",
    "            found_count_series.to_frame('tokens matched'),\n",
    "            found_count_df,\n",
    "            product_descriptor.to_frame('product_descriptor')\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # snip results table to stuff which has at least SOME match\n",
    "    found_count_summary = found_count_summary[found_count_summary['tokens matched']>0]\n",
    "    # sort on some metric\n",
    "    found_count_summary = found_count_summary.sort_values(['tokens matched']+search_tokens,ascending=False)\n",
    "\n",
    "    ###########################################################\n",
    "    # results chat\n",
    "    ###########################################################\n",
    "    if len(found_count_summary)==0:\n",
    "        print(f'results: I got nothing! T⌓T') # unhappy\n",
    "    elif len(found_count_summary)==1:\n",
    "        print(f'results: only 1 hit ￣ω￣, I hope it\\'s what you wanted!') # unsure\n",
    "    elif len(found_count_summary)<=5:\n",
    "        print(f'results: I only got {len(found_count_summary)} results, see anything you like?') # tight search\n",
    "    elif len(found_count_summary)<=10:\n",
    "        print(f'results: {len(found_count_summary)} results found') # normal\n",
    "    elif len(found_count_summary)>10:\n",
    "        print(f'results: {len(found_count_summary)} items found, I can do better if you can be more specific') # should do better\n",
    "    \n",
    "    '''\n",
    "    # tag on searched string as well for debug purposes\n",
    "    if verbose>0:\n",
    "        found_count_summary = found_count_summary.join(product_descriptor.to_frame())\n",
    "    ''' \n",
    "    \n",
    "    ###########################################################\n",
    "    # show_top_n results\n",
    "    ###########################################################\n",
    "    if show_top_n and len(found_count_summary)>0:\n",
    "        d.get_product_pictures(locs=found_count_summary.index[:show_top_n])\n",
    "        plt.show()\n",
    "\n",
    "    ###########################################################\n",
    "    # return\n",
    "    ###########################################################\n",
    "    if verbose>0:\n",
    "        return found_count_summary.head(20)\n",
    "    else:\n",
    "        return found_count_summary.head(show_top_n)\n",
    "\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['summer','short'],verbose=1) # basic logic\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['harajuku','goth','sexy'],verbose=1) # test rare token logic\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['kids','top','banana','bobby','henry'],verbose=0) # test bad search tokens\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['kids','sandals'],verbose=1) # test\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['balloon'],show_top_n=5,verbose=1) # test show_top_n\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['balloon'],show_top_n=5,verbose=1) # test results snipping\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['banana'],show_top_n=5,verbose=1) # test results chat#\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['sandal'],show_top_n=5,verbose=1) # test results chat\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['sandal','red'],show_top_n=5,verbose=1) # test results chat\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_string='sandal red',show_top_n=5,verbose=1) # test results chat\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['top','skinny','red'],show_top_n=5,verbose=1) # test results chat\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['top','skinny','red'],show_top_n=5,exact_search=True,verbose=1) # test results chat\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_string='top red',show_top_n=5,verbose=1) # test results chat\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_string='top red',show_top_n=5,exact_search=True,verbose=1) # test exact search\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_string='exy',show_top_n=5,verbose=1) # test exact search\n",
    "#tokens_found_count(product_descriptor=make_product_descriptor(),search_string='exy',show_top_n=5,exact_search=True,verbose=1) # test exact search\n",
    "tokens_found_count(product_descriptor=make_product_descriptor(),search_string='top red',show_top_n=5,exact_search=True,verbose=1) # test exact search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
