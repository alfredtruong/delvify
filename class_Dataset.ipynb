{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4599d31",
   "metadata": {},
   "source": [
    "# [TLDR] simple class to gather all data related actions in one place"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebaeaee",
   "metadata": {},
   "source": [
    "# [LONGER VERSION]\n",
    "- avoid placing data manipulations everywhere\n",
    "- data exploration done mainly in `exploration.ipynb` but also in other notebooks but prod logic is gathered here\n",
    "  - `preselected_columns` of useful data is specified here\n",
    "  - numerical data stripping, cleaning and normalizing is done here\n",
    "     1. duplicated columns removed (identified in `img_features` after seeing duplicate images)\n",
    "     2. missing data imputed with populate mean (identified in `exploration.ipynb` after failing sklearn fit_tranforms)\n",
    "     3. log-transformations to normalized data done here (reasoning in `exploration.ipynb` after looking at qqplots)\n",
    "     4. `StandardScaler` is done here (to bring data into useful space for `PCA` and `kneighbors` among other techiques)\n",
    "     5. `PCA` extraction is done here (to reduce data dimensionality, amelorate outliers and to speed up processing)\n",
    "- `product_picture` tools are gathered here\n",
    "  1. that scrape from web \n",
    "  2. builds a local cache to avoid hitting www.wish.com\n",
    "  3. `plt.imshow` plotting of `product_picture`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba59712",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ipynb_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple,Dict,List\n",
    "\n",
    "import cv2\n",
    "from urllib.request import urlretrieve\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7428c",
   "metadata": {},
   "source": [
    "# CLASS DEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c907a8ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMG                   = np.ndarray      # cv2.imread return type\n",
    "\n",
    "class Dataset():\n",
    "\n",
    "    csv = 'Sales Of Summer Cloths.csv' # put csv into jupyter notebook root dir\n",
    "    \n",
    "    preselected_columns = [\n",
    "        'title',\n",
    "        'title_orig',\n",
    "        'price',\n",
    "        'retail_price',\n",
    "        'units_sold',\n",
    "        'rating',\n",
    "        'rating_count',\n",
    "        'rating_five_count',\n",
    "        'rating_four_count',\n",
    "        'rating_three_count',\n",
    "        'rating_two_count',\n",
    "        'rating_one_count',\n",
    "        'tags',\n",
    "        'product_color',\n",
    "        'merchant_rating_count',\n",
    "        'merchant_rating',\n",
    "        'product_picture',\n",
    "    ]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        ) -> None :\n",
    "        \n",
    "        # setup directories\n",
    "        self.cwd      = Path(os.getcwd())  #'C:/Users/ahkar/OneDrive/Documents/Delvify/')\n",
    "        self.cachedir = self.cwd / 'cache' # cache dir path\n",
    "\n",
    "        # read data\n",
    "        self.raw     = self.read_csv()   # raw dataset\n",
    "        self.df      = self.clean_df()   # cleaned up dataset\n",
    "        self.df_num  = self.df_numeric() # transformed / scaled / pca'ed dataset\n",
    "        \n",
    "    # simple read\n",
    "    def read_csv(self) -> pd.DataFrame :\n",
    "        return pd.read_csv(self.cwd / Dataset.csv) # read raw df\n",
    "    \n",
    "    # remove columns deemed to be useless + duplicate listings\n",
    "    def clean_df(\n",
    "        self,\n",
    "        ) -> pd.DataFrame :\n",
    "        df = self.raw.copy()\n",
    "        df = df.loc[:,Dataset.preselected_columns] # subset to columns deemed useful\n",
    "        df = df.drop_duplicates() # remove duplicate listings\n",
    "        return df\n",
    "\n",
    "    # extract df of numeric features + populate nans\n",
    "    def df_numeric(\n",
    "        self,\n",
    "        should_impute_nans      : bool  = True,  # rather than trashing entry, populate missing ratings with population mean\n",
    "        should_log_transform    : bool  = True,  # make data distribution more normal like\n",
    "        should_standard_scale   : bool  = True,  # N(0,1) the data\n",
    "        should_minmax_scale     : bool  = False,\n",
    "        should_minmax_scale_abs : float = 2,\n",
    "        should_pca              : bool  = True,  # reduce dimensionality to speed up analysis\n",
    "        should_pca_components   : float = 6,    # elbow / eyeballed optimal principal components to use\n",
    "        ) -> pd.DataFrame :\n",
    "        # strip\n",
    "        df_num = self.df.select_dtypes(include=['int64','float64']).copy()\n",
    "        \n",
    "        # impute nans\n",
    "        if should_impute_nans:\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            imputed = imp.fit_transform(df_num)\n",
    "            df_num = pd.DataFrame(imputed,index=df_num.index,columns=df_num.columns)\n",
    "    \n",
    "        # apply log transform\n",
    "        if should_log_transform:\n",
    "            df_num = np.log(df_num+1.1)\n",
    "            \n",
    "        # apply standard scaler\n",
    "        if should_standard_scale:\n",
    "            s_scaler = StandardScaler()\n",
    "            scaled = s_scaler.fit_transform(df_num)\n",
    "            df_num = pd.DataFrame(scaled,index=df_num.index,columns=df_num.columns)\n",
    "            \n",
    "        # apply min max scaler\n",
    "        if should_minmax_scale:\n",
    "            mm_scaler = MinMaxScaler(feature_range=(should_minmax_scale_abs*-1,should_minmax_scale_abs))\n",
    "            scaled = mm_scaler.fit_transform(df_num)\n",
    "            df_num = pd.DataFrame(scaled,index=df_num.index,columns=df_num.columns)\n",
    "        \n",
    "        # apply pca\n",
    "        if should_pca:\n",
    "            pca = PCA(n_components=should_pca_components)\n",
    "            reduced = pca.fit_transform(df_num)\n",
    "            df_num = pd.DataFrame(reduced,index=df_num.index)\n",
    "            df_num.columns = ['ev'+str(x) for x in df_num.columns] # rename columns\n",
    "\n",
    "        # return\n",
    "        return df_num\n",
    "\n",
    "    def url_to_cache_filepath(\n",
    "        self,\n",
    "        url : str, # = 'https://contestimg.wish.com/api/webimage/5e9ae51d43d6a96e303acdb0-medium.jpg',\n",
    "        ) -> Path :\n",
    "        return self.cachedir / Path(url).name\n",
    "\n",
    "    # return jpg associated with url in product_picture\n",
    "    def get_product_picture(\n",
    "        self,\n",
    "        url        : str            = None,  # 'https://contestimg.wish.com/api/webimage/5e9ae51d43d6a96e303acdb0-medium.jpg'\n",
    "        loc        : int            = None,  # 0 / index name to read\n",
    "        plot       : bool           = True,  # should plot final result\n",
    "        grayscale  : bool           = False, # to avoid finding same item in different colour\n",
    "        blur       : bool           = False, # to avoid finding same item with minor difference in listing\n",
    "        blur_ksize : Tuple[int,int] = (5,5), # the larger the stronger the smoothing\n",
    "        verbose    : int            = 0,     # give details\n",
    "        ) -> IMG :\n",
    "        '''\n",
    "        d=Dataset() # instantiate\n",
    "        d.get_product_picture(url='https://contestimg.wish.com/api/webimage/5e9ae51d43d6a96e303acdb0-medium.jpg')\n",
    "        d.get_product_picture(loc=3)\n",
    "        d.get_product_picture(loc=3,blur=False,grayscale=True)\n",
    "        '''\n",
    "        \n",
    "        ####################################\n",
    "        # ensure url is populated\n",
    "        ####################################\n",
    "        if loc is not None:\n",
    "            url = self.df['product_picture'].loc[loc]\n",
    "            \n",
    "        ####################################\n",
    "        # ensure local cache populated\n",
    "        ####################################\n",
    "        local_filepath = self.url_to_cache_filepath(url) # target location\n",
    "        if local_filepath.exists():\n",
    "            if verbose>0: print(f'read {local_filepath}')\n",
    "        else:\n",
    "            if not self.cachedir.exists(): os.mkdir(self.cachedir) # ensure cache dir exists\n",
    "            urlretrieve(url,local_filepath) # populate local cache each time url / jpg is requested\n",
    "            if verbose>0: print(f'cache {url}')\n",
    "        \n",
    "        ####################################\n",
    "        # read img from cache\n",
    "        ####################################\n",
    "        # read raw file\n",
    "        img = cv2.imread(str(local_filepath))\n",
    "        \n",
    "        # should apply grayscale?\n",
    "        if grayscale:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # yes\n",
    "            img = np.stack([img,img,img],axis=2)\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # no, then just do default RGB\n",
    "        \n",
    "        # should apply blur?\n",
    "        if blur:\n",
    "            img = cv2.blur(img, blur_ksize) \n",
    "\n",
    "        # should plot?\n",
    "        if plot:\n",
    "            fig,ax = plt.subplots(1,1)\n",
    "            ax.imshow(img)\n",
    "            ax.set_axis_off()\n",
    "            ax.set_title(loc)\n",
    "            plt.show()     \n",
    "\n",
    "        # return\n",
    "        return img\n",
    "\n",
    "    # force populate cache with all imgs from urls in df['product_picture']\n",
    "    def populate_cache(self):\n",
    "        '''\n",
    "        d=Dataset() # instantiate\n",
    "        d.populate_cache() # force read on all product_picture urls\n",
    "        '''\n",
    "        Parallel(n_jobs=-1)(delayed(self.get_product_picture)(url) for url in self.df['product_picture']) # batch download images\n",
    "\n",
    "    # return multiple jpgs associated locs\n",
    "    def get_product_pictures(\n",
    "        self,\n",
    "        locs       : List[int]      = None,  # 0 / index name to read\n",
    "        ) -> IMG :\n",
    "        '''\n",
    "        d.get_product_pictures(locs=[1307,758,1183,1516])\n",
    "        '''\n",
    "        # gather imgs\n",
    "        imgs = [self.get_product_picture(loc=loc,plot=False) for loc in locs]\n",
    "        \n",
    "        def ceildiv(a,b): return -(a // -b) # ceiling division without needing any imports\n",
    "\n",
    "        # prep plotting device\n",
    "        MAX_COLS   = 5\n",
    "        PLOT_WIDTH = 5\n",
    "        plot_cols = min(5,len(locs))\n",
    "        plot_rows = ceildiv(len(locs),plot_cols) # max number of rows needed to plot 'locs' pictures with 'cols' pictures on each row\n",
    "        fig,ax = plt.subplots(plot_rows,plot_cols,figsize=(PLOT_WIDTH * plot_cols,PLOT_WIDTH * plot_rows))\n",
    "\n",
    "        # plot imgs\n",
    "        curr_plotting_row = -1 # increments to 0 on first loop as increment condition is true\n",
    "        for i,(loc,img) in enumerate(zip(locs,imgs)):\n",
    "            # plotting column location\n",
    "            curr_plotting_col = i%plot_cols\n",
    "            # increment plotting row each time we hit the 'cols'th column\n",
    "            if curr_plotting_col == 0:\n",
    "                curr_plotting_row = curr_plotting_row + 1\n",
    "                \n",
    "            # plot\n",
    "            if plot_rows == 1:\n",
    "                if plot_cols == 1:\n",
    "                    # dont need any dimension if only 1 plot\n",
    "                    ax.imshow(img) \n",
    "                    ax.set_axis_off()\n",
    "                    ax.set_title(loc)\n",
    "                else:\n",
    "                    # dont need 2nd dimension for ax if only 1 row\n",
    "                    ax[curr_plotting_col].imshow(img) \n",
    "                    ax[curr_plotting_col].set_axis_off() \n",
    "                    ax[curr_plotting_col].set_title(loc)\n",
    "            else:\n",
    "                # need 2nd dimension for ax if more than 1 row\n",
    "                ax[curr_plotting_row,curr_plotting_col].imshow(img)\n",
    "                ax[curr_plotting_row,curr_plotting_col].set_axis_off() # \n",
    "                ax[curr_plotting_row,curr_plotting_col].set_title(loc)\n",
    "\n",
    "    # plot top n index from df\n",
    "    def show_top_n(\n",
    "        self,\n",
    "        res : pd.DataFrame,\n",
    "        n   : int = 5\n",
    "        ) -> pd.DataFrame:\n",
    "        if len(res)>0: self.get_product_pictures(locs=res.index[:n]);plt.show()\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baffa665",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "d=Dataset()\n",
    "print(d.raw.shape)\n",
    "print(d.df.shape)\n",
    "print(d.df_num.shape)\n",
    "d.get_product_picture(loc=20);\n",
    "d.get_product_pictures(locs=[20,21,22]);\n",
    "d.df_numeric(\n",
    "        should_impute_nans       = True,  # rather than trashing entry, populate missing ratings with population mean\n",
    "        should_log_transform     = True,  # make data distribution more normal like\n",
    "        should_standard_scale    = True,  # N(0,1) the data\n",
    "        should_minmax_scale      = False,\n",
    "        should_minmax_scale_abs  = 2,\n",
    "        should_pca               = True,  # reduce dimensionality to speed up analysis\n",
    "        should_pca_components     = 6,    # elbow / eyeballed optimal principal components to use\n",
    ")\n",
    "'''\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
