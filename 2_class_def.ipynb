{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba59712",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1966c595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run notebook_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from joblib import memory\n",
    "\n",
    "from typing import Tuple,Dict\n",
    "\n",
    "import cv2\n",
    "import image_similarity_measures\n",
    "from image_similarity_measures.quality_metrics import fsim,issm,psnr,rmse,sam,sre,ssim,uiq \n",
    "# https://github.com/up42/image-similarity-measures\n",
    "# https://up42.com/blog/tech/image-similarity-measures\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7428c",
   "metadata": {},
   "source": [
    "# CLASS DEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c907a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG                   = np.ndarray      # cv2.imread return type\n",
    "IMG_SIMILARITY_DICT   = Dict[str,float] # k,v dict with k = measure and v = similarity metric\n",
    "IMG_SIMILARITY_DF     = pd.DataFrame    # cols of similarity metrics\n",
    "IMG_SIMILARITY_SERIES = pd.Series       # column from IMG_SIMILARITY_DF\n",
    "NOTEBOOK_PLOTS        = None\n",
    "\n",
    "class Dataset():\n",
    "\n",
    "    # raw data\n",
    "    csv = 'Sales Of Summer Cloths.csv'\n",
    "    \n",
    "    preselected_columns = [\n",
    "        'title',\n",
    "        'title_orig',\n",
    "        'price',\n",
    "        'retail_price',\n",
    "        'units_sold',\n",
    "        'rating',\n",
    "        'rating_count',\n",
    "        'rating_five_count',\n",
    "        'rating_four_count',\n",
    "        'rating_three_count',\n",
    "        'rating_two_count',\n",
    "        'rating_one_count',\n",
    "        'tags',\n",
    "        'product_color',\n",
    "        'merchant_title',\n",
    "        'merchant_name',\n",
    "        'merchant_rating_count',\n",
    "        'merchant_rating',\n",
    "        'product_picture',\n",
    "    ]\n",
    "    \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        do_fsim = False, # Feature-based similarity index (FSIM)                                   # 0 to 1, 0 == bad, 1 == identical                      # ascending=False\n",
    "        do_issm = False, # Information theoretic-based Statistic Similarity Measure (ISSM)\n",
    "        do_psnr = True,  # Peak signal-to-noise ratio (PSNR)                                                                                               # ascending=False\n",
    "        do_rmse = True,  # Root mean square error (RMSE)                                           # 0 to inf, 0 == identical, the smaller the better      # ascending=True\n",
    "        do_sam  = True,  # Spectral angle mapper (SAM)                                                                                                     # ascending=False\n",
    "        do_sre  = True,  # Signal to reconstruction error ratio (SRE)                                                                                      # ascending=False\n",
    "        do_ssim = True,  # Structural Similarity Index (SSIM)                                      # -1 to 1, -1 == bad, 1 == good, the larger the better  # ascending=False\n",
    "        do_uiq  = False, # Universal image quality index (UIQ)                                     # -1 to 1, -1 == bad, 1 == good, the larger the better  # ascending=False\n",
    "        ) -> None :\n",
    "        \n",
    "        # setup directories\n",
    "        self.cwd      = Path(os.getcwd())  #'C:/Users/ahkar/OneDrive/Documents/Delvify/')\n",
    "        self.cachedir = self.cwd / 'cache' # cache dir path\n",
    "\n",
    "        # differences\n",
    "        self.do_fsim = do_fsim # Feature-based similarity index (FSIM)\n",
    "        self.do_issm = do_issm # Information theoretic-based Statistic Similarity Measure (ISSM)\n",
    "        self.do_psnr = do_psnr # Peak signal-to-noise ratio (PSNR)\n",
    "        self.do_rmse = do_rmse # Root mean square error (RMSE)\n",
    "        self.do_sam  = do_sam  # Spectral angle mapper (SAM)\n",
    "        self.do_sre  = do_sre  # Signal to reconstruction error ratio (SRE)\n",
    "        self.do_ssim = do_ssim # Structural Similarity Index (SSIM)\n",
    "        self.do_uiq  = do_uiq  # Universal image quality index (UIQ)\n",
    "\n",
    "        # read data\n",
    "        self.raw     = self.read_csv() # simple read\n",
    "        self.df      = self.clean_df()\n",
    "        \n",
    "        # precompute k means\n",
    "        self.kmean_groups = self.precompute_kmeans()\n",
    "        \n",
    "    # simple read\n",
    "    def read_csv(self) -> pd.DataFrame :\n",
    "        return pd.read_csv(self.cwd / Dataset.csv) # read raw df\n",
    "    \n",
    "    # remove columns deemed to be useless + duplicate listings\n",
    "    def clean_df(\n",
    "        self,\n",
    "        ) -> pd.DataFrame :\n",
    "        df = self.raw.copy()\n",
    "        df = df.loc[:,Dataset.preselected_columns] # subset to columns deemed useful\n",
    "        df = df.drop_duplicates() # remove duplicate listings\n",
    "        return df\n",
    "\n",
    "    # extract df of numeric features + populate nans\n",
    "    def df_num(\n",
    "        self,\n",
    "        should_impute_nans      : bool = True,\n",
    "        should_standard_scale   : bool = True,\n",
    "        #should_minmax_scale     : bool = True,\n",
    "        #should_minmax_scale_abs : float = 2,\n",
    "        should_pca              : bool = True,\n",
    "        should_pca_components   : float = 6,\n",
    "        ) -> pd.DataFrame :\n",
    "        # strip\n",
    "        df_num = self.df.select_dtypes(include=['int64','float64']).copy()\n",
    "        \n",
    "        # impute nans\n",
    "        if should_impute_nans:\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            imputed = imp.fit_transform(df_num)\n",
    "            df_num = pd.DataFrame(imputed,index=df_num.index,columns=df_num.columns)\n",
    "    \n",
    "        # apply standard scaler\n",
    "        if should_standard_scale:\n",
    "            s_scaler = StandardScaler()\n",
    "            scaled = s_scaler.fit_transform(df_num)\n",
    "            df_num = pd.DataFrame(scaled,index=df_num.index,columns=df_num.columns)\n",
    "            \n",
    "        '''\n",
    "        # apply min max scaler\n",
    "        if should_minmax_scale:\n",
    "            mm_scaler = MinMaxScaler(feature_range=(should_minmax_scale_abs*-1,should_minmax_scale_abs))\n",
    "            scaled = mm_scaler.fit_transform(df_num)\n",
    "            df_num = pd.DataFrame(scaled,index=df_num.index,columns=df_num.columns)\n",
    "        ''' \n",
    "        # apply pca\n",
    "        if should_pca:\n",
    "            pca = PCA(n_components=should_pca_components)\n",
    "            reduced = pca.fit_transform(df_num)\n",
    "            df_num = pd.DataFrame(reduced)\n",
    "            df_num.columns = ['ev'+str(x) for x in df_num.columns] # rename columns\n",
    "\n",
    "        # return\n",
    "        return df_num\n",
    "\n",
    "    def precompute_kmeans(\n",
    "        self,\n",
    "        ) -> np.ndarray :\n",
    "        # prep object              \n",
    "        kmeans = KMeans(\n",
    "            init         = \"random\",\n",
    "            n_clusters   = 10, # seems use to residuals are pretty good already for 10 groups\n",
    "            n_init       = 10,\n",
    "            max_iter     = 300,\n",
    "            random_state = 42\n",
    "        )\n",
    "\n",
    "        kmeans.fit(self.df_num()) # apply kmeans on scaled features cos kmeans metric sensitive to scale\n",
    "\n",
    "        return kmeans.labels_ # predictions\n",
    "\n",
    "    def url_to_cache_filepath(\n",
    "        self,\n",
    "        url : str, # = 'https://contestimg.wish.com/api/webimage/5e9ae51d43d6a96e303acdb0-medium.jpg',\n",
    "        ) -> Path :\n",
    "        return self.cachedir / Path(url).name\n",
    "\n",
    "    # return multiple jpgs associated locs\n",
    "    def get_product_pictures(\n",
    "        self,\n",
    "        locs       : List[int]      = None,  # 0 / index name to read\n",
    "        ) -> IMG :\n",
    "        '''\n",
    "        d.get_product_pictures(locs=[1307,758,1183,1516])\n",
    "        '''\n",
    "        # gather imgs\n",
    "        imgs = [self.get_product_picture(loc=loc,plot=False) for loc in locs]\n",
    "        \n",
    "        def ceildiv(a,b): return -(a // -b) # ceiling division without needing any imports\n",
    "\n",
    "        # prep plotting device\n",
    "        MAX_COLS   = 5\n",
    "        PLOT_WIDTH = 5\n",
    "        plot_cols = min(5,len(locs))\n",
    "        plot_rows = ceildiv(len(locs),plot_cols) # max number of rows needed to plot 'locs' pictures with 'cols' pictures on each row\n",
    "        fig,ax = plt.subplots(plot_rows,plot_cols,figsize=(PLOT_WIDTH * plot_cols,PLOT_WIDTH * plot_rows))\n",
    "\n",
    "        # plot imgs\n",
    "        curr_plotting_row = -1 # increments to 0 on first loop as increment condition is true\n",
    "        for i,img in enumerate(imgs):\n",
    "            # plotting column location\n",
    "            curr_plotting_col = i%plot_cols\n",
    "            # increment plotting row each time we hit the 'cols'th column\n",
    "            if curr_plotting_col == 0:\n",
    "                curr_plotting_row = curr_plotting_row + 1\n",
    "                \n",
    "            # plot\n",
    "            if plot_rows == 1:\n",
    "                ax[curr_plotting_col].imshow(img) # dont need 2nd dimension for ax if only 1 row\n",
    "            else:\n",
    "                ax[curr_plotting_row,curr_plotting_col].imshow(img) # need 2nd dimension for ax if more than 1 row\n",
    "\n",
    "        # remove axes from ALL ax's\n",
    "        if plot_rows == 1:\n",
    "            for curr_plotting_col in range(plot_cols):\n",
    "                ax[curr_plotting_col].set_axis_off()\n",
    "        else:\n",
    "            for curr_plotting_row in range(plot_rows):\n",
    "                for curr_plotting_col in range(plot_cols):\n",
    "                    ax[curr_plotting_row,curr_plotting_col].set_axis_off()\n",
    "\n",
    "\n",
    "    # return jpg associated with url in product_picture\n",
    "    def get_product_picture(\n",
    "        self,\n",
    "        url        : str            = None,  # 'https://contestimg.wish.com/api/webimage/5e9ae51d43d6a96e303acdb0-medium.jpg'\n",
    "        loc        : int            = None,  # 0 / index name to read\n",
    "        plot       : bool           = True,  # should plot final result\n",
    "        grayscale  : bool           = False, # to avoid finding same item in different colour\n",
    "        blur       : bool           = False, # to avoid finding same item with minor difference in listing\n",
    "        blur_ksize : Tuple[int,int] = (5,5), # the larger the stronger the smoothing\n",
    "        verbose    : int            = 0,     # give details\n",
    "        ) -> IMG :\n",
    "        '''\n",
    "        d=Dataset() # instantiate\n",
    "        d.get_product_picture(url='https://contestimg.wish.com/api/webimage/5e9ae51d43d6a96e303acdb0-medium.jpg')\n",
    "        d.get_product_picture(loc=3)\n",
    "        d.get_product_picture(loc=3,blur=False,grayscale=True)\n",
    "        '''\n",
    "        \n",
    "        ####################################\n",
    "        # ensure url is populated\n",
    "        ####################################\n",
    "        if loc is not None:\n",
    "            url = d.df['product_picture'].loc[loc]\n",
    "            \n",
    "        ####################################\n",
    "        # ensure local cache populated\n",
    "        ####################################\n",
    "        local_filepath = self.url_to_cache_filepath(url) # target location\n",
    "        if local_filepath.exists():\n",
    "            if verbose>0: print(f'read {local_filepath}')\n",
    "        else:\n",
    "            if not self.cachedir.exists(): os.mkdir(self.cachedir) # ensure cache dir exists\n",
    "            urlretrieve(url,local_filepath) # populate local cache each time url / jpg is requested\n",
    "            if verbose>0: print(f'cache {url}')\n",
    "        \n",
    "        ####################################\n",
    "        # read img from cache\n",
    "        ####################################\n",
    "        # read raw file\n",
    "        img = cv2.imread(str(local_filepath))\n",
    "        \n",
    "        # should apply grayscale?\n",
    "        if grayscale:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # yes\n",
    "            img = np.stack([img,img,img],axis=2)\n",
    "        else:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # no, then just do default RGB\n",
    "        \n",
    "        # should apply blur?\n",
    "        if blur:\n",
    "            img = cv2.blur(img, blur_ksize) \n",
    "\n",
    "        # should plot?\n",
    "        if plot: plt.imshow(img);plt.show()\n",
    "\n",
    "        # return\n",
    "        return img\n",
    "\n",
    "    # force populate cache with all imgs from urls in df['product_picture']\n",
    "    def populate_cache(self):\n",
    "        '''\n",
    "        d=Dataset() # instantiate\n",
    "        d.populate_cache() # force read on all product_picture urls\n",
    "        '''\n",
    "        Parallel(n_jobs=-1)(delayed(self.get_product_picture)(url) for url in self.df['product_picture']) # batch download images\n",
    "\n",
    "    # compute similarity metrics between some input image and some other image\n",
    "    def img_similarity_pair(\n",
    "        self,\n",
    "        tgt_loc     : int,\n",
    "        src_loc     : int            = None,  # one of src_loc or src_img need to be given\n",
    "        src_img     : IMG            = None,  # one of src_loc or src_img need to be given\n",
    "        plot_src    : bool           = True,  # should plot src img\n",
    "        plot_tgt    : bool           = True,  # should plot tgt img\n",
    "        grayscale   : bool           = False, # to avoid finding same item in different colour\n",
    "        blur        : bool           = False, # to avoid finding same item with minor difference in listing\n",
    "        blur_ksize  : Tuple[int,int] = (5,5), # the larger the stronger the smoothing\n",
    "        ) -> IMG_SIMILARITY_DICT :\n",
    "        '''\n",
    "        d.img_similarity_pair(src_loc=3,tgt_loc=0)\n",
    "        '''\n",
    "        print(f'tgt_loc = {tgt_loc}')\n",
    "        \n",
    "        #####################################\n",
    "        # srcimg\n",
    "        #####################################\n",
    "        # get srcimg\n",
    "        if src_loc is not None:\n",
    "            src_img = self.get_product_picture(\n",
    "                loc        = src_loc,\n",
    "                plot       = plot_src,\n",
    "                grayscale  = grayscale,\n",
    "                blur       = blur,\n",
    "                blur_ksize = blur_ksize,\n",
    "            )\n",
    "        \n",
    "        # srcimg scale\n",
    "        scale_pct = 100 # percent of original img size\n",
    "        src_img_width  = int(src_img.shape[1] * scale_pct / 100)\n",
    "        src_img_height = int(src_img.shape[0] * scale_pct / 100)\n",
    "        src_img_dim    = (src_img_width, src_img_height)\n",
    "\n",
    "        #####################################\n",
    "        # get other_img\n",
    "        #####################################\n",
    "        tgt_img = self.get_product_picture(\n",
    "            loc        = tgt_loc,\n",
    "            plot       = plot_tgt,\n",
    "            grayscale  = grayscale,\n",
    "            blur       = blur,\n",
    "            blur_ksize = blur_ksize,\n",
    "        )\n",
    "\n",
    "        # resize to srcimg size\n",
    "        resized_tgt_img = cv2.resize(tgt_img, src_img_dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        #####################################\n",
    "        # compute distances\n",
    "        #####################################\n",
    "        out = {}\n",
    "        if self.do_fsim: out['fsim'] = fsim(src_img, resized_tgt_img)\n",
    "        if self.do_issm: out['issm'] = issm(src_img, resized_tgt_img)\n",
    "        if self.do_psnr: out['psnr'] = psnr(src_img, resized_tgt_img)\n",
    "        if self.do_rmse: out['rmse'] = rmse(src_img, resized_tgt_img)\n",
    "        if self.do_sam:  out['sam']  = sam(src_img,  resized_tgt_img)\n",
    "        if self.do_sre:  out['sre']  = sre(src_img,  resized_tgt_img)\n",
    "        if self.do_ssim: out['ssim'] = ssim(src_img, resized_tgt_img)\n",
    "        if self.do_uiq:  out['uiq']  = uiq(src_img,  resized_tgt_img)\n",
    "\n",
    "        #####################################\n",
    "        # return dict\n",
    "        #####################################\n",
    "        return out\n",
    "    \n",
    "    # compute similarity of some input image to all other images\n",
    "    def img_similarity_all(\n",
    "        self,\n",
    "        loc         : int            = None,  # 0 / index name to read\n",
    "        plot_src    : bool           = True,  # should plot src img\n",
    "        plot_tgt    : bool           = False, # should plot tgt img\n",
    "        grayscale   : bool           = False, # to avoid finding same item in different colour\n",
    "        blur        : bool           = False, # to avoid finding same item with minor difference in listing\n",
    "        blur_ksize  : Tuple[int,int] = (5,5), # the larger the stronger the smoothing\n",
    "        do_parallel : bool           = True,\n",
    "        ) -> IMG_SIMILARITY_DF :\n",
    "        \n",
    "        #####################################\n",
    "        # src_img\n",
    "        #####################################\n",
    "        # get src_img\n",
    "        src_img = self.get_product_picture(\n",
    "            loc        = loc,\n",
    "            plot       = plot_src,\n",
    "            grayscale  = grayscale,\n",
    "            blur       = blur,\n",
    "            blur_ksize = blur_ksize,\n",
    "        )\n",
    "                \n",
    "        #####################################\n",
    "        # function to compute distances\n",
    "        #####################################\n",
    "        def JOBLIB_PARALLEL_FUNC(\n",
    "            tgt_loc : int,\n",
    "            ) -> IMG_SIMILARITY_DICT :\n",
    "            return self.img_similarity_pair(\n",
    "                tgt_loc    = tgt_loc,\n",
    "                src_loc    = None,\n",
    "                src_img    = src_img,\n",
    "                plot_src   = plot_src,\n",
    "                plot_tgt   = plot_tgt,\n",
    "                grayscale  = grayscale,\n",
    "                blur       = blur,\n",
    "                blur_ksize = blur_ksize,\n",
    "            )\n",
    "        \n",
    "        # parallelize the compute\n",
    "        if do_parallel:\n",
    "            out = Parallel(n_jobs=-1)(delayed(JOBLIB_PARALLEL_FUNC)(tgt_loc) for tgt_loc in self.df.index)\n",
    "        else:\n",
    "            out = [JOBLIB_PARALLEL_FUNC(tgt_loc) for tgt_loc in self.df.index]\n",
    "        \n",
    "        #####################################\n",
    "        # return df\n",
    "        #####################################\n",
    "        return pd.DataFrame(out,index=self.df.index)\n",
    "\n",
    "    # show distribution of image similarity metrics for our dataset\n",
    "    def img_similarity_plot(\n",
    "        self,\n",
    "        img_similarity_df : IMG_SIMILARITY_DF,\n",
    "        ) -> NOTEBOOK_PLOTS :\n",
    "        df = img_similarity_df.copy()\n",
    "        df = df.replace([np.inf, -np.inf],0)\n",
    "        df.hist(figsize=(20,15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
