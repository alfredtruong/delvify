{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2073bf9e",
   "metadata": {},
   "source": [
    "# [TLDR] match search tokens to some `product_descriptor` for each product\n",
    "\n",
    "- token matching seems most direct in grabbing home truths re what the customer is looking for\n",
    "\n",
    "- upon beign given a `search_string`, simple regex magic\n",
    "  - is done to partition it into `token`s\n",
    "  - `token` matching to the `product_descriptor` can then be done either by\n",
    "    - exact matching (with leading and trailing spaces) or \n",
    "    - simply on presence (is good to catch poorly partitioned `tags`)\n",
    "  - matching can also be\n",
    "    - case sensitive or\n",
    "    - case insenstive\n",
    "  \n",
    "- the `product_descriptor`\n",
    "  - is built from all string like info that is predeemed to be interesting, namely\n",
    "    - `title` (~ French description)\n",
    "    - `title_orig` (~ English translation though as the translation could be wrong there is no harm in doubling up)\n",
    "    - `tags` (~ non-standard category labelling of the product)\n",
    "      - is sometimes partitioned with space and ;\n",
    "      - and others appears as a continuous string (non-exact matching is better for this)\n",
    "    - `product_color` (self explanatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba59712",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e201c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run ipynb_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run class_Dataset.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe30f19",
   "metadata": {},
   "source": [
    "# IMPLEMENT TEXT SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135010e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68678f",
   "metadata": {},
   "source": [
    "# FUNCTIONS TO BUILD UP QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb792c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_PATTERN      = str\n",
    "COLNAME            = str\n",
    "PRODUCT_DESCRIPTOR = pd.Series(dtype='str')\n",
    "SEARCH_STRING      = str\n",
    "TOKENS             = str\n",
    "SEARCH_TOKENS      = List[str]\n",
    "\n",
    "class TokenSearch():\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset : Dataset,\n",
    "        ) -> None :\n",
    "        self.dataset            = dataset\n",
    "        self.product_descriptor = self.make_product_descriptor()\n",
    "        \n",
    "    def make_product_descriptor(\n",
    "        self,\n",
    "        descriptor_colnames : List[COLNAME] = ['title','title_orig','tags','product_color'] # cols of interest\n",
    "        ) -> PRODUCT_DESCRIPTOR :\n",
    "        '''\n",
    "        TokenSearch.make_product_descriptor()\n",
    "        '''\n",
    "        descriptors = self.dataset.df[descriptor_colnames] # extract columns\n",
    "        descriptors = descriptors.apply(lambda x:' '.join([str(x) for x in x.values]),axis=1) # merge with | as sep\n",
    "        #descriptors = descriptors.str.replace('\\'','') # strip appostrophes\n",
    "        descriptors = descriptors.str.replace('\\W',' ',regex=True) # strip special chars\n",
    "        return descriptors\n",
    "\n",
    "    # search string tokenizer\n",
    "    def tokenize_search_string(\n",
    "        self,\n",
    "        search_string : SEARCH_STRING,\n",
    "        exact_match   : bool = False,\n",
    "        ) -> SEARCH_TOKENS :\n",
    "        tokens = re.split('\\W+',search_string) # partition string on non-alphanumeric chars\n",
    "        #tokens = [token.lower() for token in tokens] # set tokens to lower case\n",
    "        tokens = list(set(tokens)) # uniques\n",
    "        if exact_match: return ['\\\\b'+token+'\\\\b' for token in tokens] # make tokens exact matches\n",
    "        return tokens\n",
    "\n",
    "    def build_regex_from_tokens(\n",
    "        self,\n",
    "        search_tokens : SEARCH_TOKENS,\n",
    "        ) -> REGEX_PATTERN :\n",
    "        base = r'^{}'\n",
    "        expr = '(?=.*{})'\n",
    "        return base.format(''.join(expr.format(w) for w in search_tokens))\n",
    "    #display(build_regex(ts.tokenize_search_string('RED banana')))\n",
    "    \n",
    "    def tokens_found_count(\n",
    "        self,\n",
    "        search_tokens      : List[TOKENS]  = None, # can give either search_string or search_tokens\n",
    "        search_string      : SEARCH_STRING = None, # can give either search_string or search_tokens\n",
    "        exact_match        : bool          = False,\n",
    "        case_sensitive     : bool          = False,\n",
    "        verbose            : int           = 0, # show workings\n",
    "        ) -> pd.Series(dtype='int') :\n",
    "        ###########################################################\n",
    "        # build regex for pd.Series.str.contains()\n",
    "        ###########################################################\n",
    "        # get search tokens if not already given\n",
    "        if search_tokens is None:\n",
    "            search_tokens = self.tokenize_search_string(\n",
    "                search_string = search_string,\n",
    "                exact_match   = exact_match,\n",
    "            )\n",
    "        if verbose>1: print(f'search_tokens = {search_tokens}')\n",
    "\n",
    "        # compile regex\n",
    "        regex_str = self.build_regex_from_tokens(search_tokens)\n",
    "        \n",
    "        ###########################################################\n",
    "        # apply regex to product_descriptor\n",
    "        ###########################################################\n",
    "        # returns a List[pd.Series(dtype=bool)], each list entry denoteing if nth token is found (exact or simple matching)\n",
    "        if case_sensitive:\n",
    "            found_series = self.product_descriptor.str.contains(regex_str)\n",
    "        else:\n",
    "            found_series = self.product_descriptor.str.contains(regex_str,flags=re.IGNORECASE)\n",
    "        \n",
    "        ###########################################################\n",
    "        # make pretty return df\n",
    "        ###########################################################\n",
    "        res = pd.concat(\n",
    "            [\n",
    "                self.product_descriptor.to_frame('product_descriptor'),\n",
    "                self.dataset.df, #found_series.to_frame('found'),\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # snip results table to stuff which has at least SOME match\n",
    "        res = res[found_series]\n",
    "        \n",
    "        ###########################################################\n",
    "        # return\n",
    "        ###########################################################\n",
    "        if verbose>0:\n",
    "            return res.head(20)\n",
    "        else:\n",
    "            return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d01d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "d=Dataset()\n",
    "ts=TokenSearch(dataset=d)\n",
    "d.show_top_n(ts.tokens_found_count(search_tokens=['summer','short'],verbose=1)) # basic logic\n",
    "d.show_top_n(ts.tokens_found_count(search_tokens=['harajuku'],verbose=1)) # test rare token logic\n",
    "d.show_top_n(ts.tokens_found_count(search_tokens=['harajuku','sexy'],verbose=1)) # test rare token logic\n",
    "d.show_top_n(ts.tokens_found_count(search_tokens=['harajuku','goth','sexy'],verbose=1)) # test rare token logic\n",
    "d.show_top_n(ts.tokens_found_count(search_tokens=['kids'],verbose=1)) # test\n",
    "d.show_top_n(ts.tokens_found_count(search_tokens=['kids','ball'],verbose=1)) # test\n",
    "d.show_top_n(ts.tokens_found_count(search_tokens=['balloon'],verbose=1)) # test show_top_n\n",
    "d.show_top_n(ts.tokens_found_count(search_tokens=['balloon'],verbose=1)) # test results snipping\n",
    "d.show_top_n(ts.tokens_found_count(search_tokens=['banana'],verbose=1)) # test results chat#\n",
    "d.show_top_n(ts.tokens_found_count(search_tokens=['sandal'],verbose=1)) # test results chat\n",
    "d.show_top_n(ts.tokens_found_count(search_tokens=['sandal','red'],verbose=1)) # test results chat\n",
    "d.show_top_n(ts.tokens_found_count(search_string='sandal red',verbose=1)) # test results chat\n",
    "d.show_top_n(ts.tokens_found_count(search_tokens=['top','skinny','red'],verbose=1)) # test results chat\n",
    "d.show_top_n(ts.tokens_found_count(search_tokens=['top','skinny','red'],exact_match=True,verbose=1)) # test results chat\n",
    "d.show_top_n(ts.tokens_found_count(search_string='top red',verbose=1)) # test results chat\n",
    "d.show_top_n(ts.tokens_found_count(search_string='top red',exact_match=True,verbose=1)) # test exact search\n",
    "d.show_top_n(ts.tokens_found_count(search_string='exy',verbose=1)) # test exact search\n",
    "d.show_top_n(ts.tokens_found_count(search_string='exy',exact_match=True,verbose=1)) # test exact search\n",
    "d.show_top_n(ts.tokens_found_count(search_string='top red',exact_match=True,verbose=1)) # test exact search\n",
    "d.show_top_n(ts.tokens_found_count(search_string='hot',exact_match=True,verbose=1)) # test exact search\n",
    "d.show_top_n(ts.tokens_found_count(search_string='hot skirt',exact_match=True,verbose=1)) # test non existent search\n",
    "d.show_top_n(ts.tokens_found_count(search_string='hot skin',exact_match=True,verbose=1)) # test exact search\n",
    "d.show_top_n(ts.tokens_found_count(search_string='hot skinny',exact_match=True,verbose=1)) # test exact search\n",
    "d.show_top_n(ts.tokens_found_count(search_string='hot skinny',exact_match=False,verbose=1)) # test exact search\n",
    "d.show_top_n(ts.tokens_found_count(search_string='skirt slim',exact_match=False,verbose=0)) # test exact search\n",
    "d.show_top_n(ts.tokens_found_count(search_string='skirt slim',exact_match=False,verbose=0)) # no plotting\n",
    "d.show_top_n(ts.tokens_found_count(search_string='hot',case_sensitive=True,verbose=1)) # test case sensitive\n",
    "d.show_top_n(ts.tokens_found_count(search_string='VANGULL',case_sensitive=True,verbose=1)) # test case sensitive\n",
    "d.show_top_n(ts.tokens_found_count(search_string='VANGULL banana',case_sensitive=True,verbose=1)) # test case sensitive\n",
    "d.show_top_n(ts.tokens_found_count(search_string='VANGULl',case_sensitive=True,verbose=1)) # test case sensitive\n",
    "d.show_top_n(ts.tokens_found_count(search_string='VANGUL',case_sensitive=True,verbose=1)) # test case sensitive\n",
    "d.show_top_n(ts.tokens_found_count(search_string='VANGUL',case_sensitive=True,exact_match=True,verbose=1)) # test case sensitive\n",
    "d.show_top_n(ts.tokens_found_count(search_string='hot',case_sensitive=True,verbose=1)) # test case sensitive\n",
    "'''\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
