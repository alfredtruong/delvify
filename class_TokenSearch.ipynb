{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2073bf9e",
   "metadata": {},
   "source": [
    "# [TLDR] PROGRESSIVELY BUILT UP A TEXT SEARCH\n",
    "- main points include\n",
    "\n",
    "1. input `search_string` broken up (tokenized) on `\\W` special chars\n",
    "\n",
    "2. can look for tokens in \"product descriptor\" either by (a) exact matching tokens OR (b) simple matching of tokens\n",
    "- exact matching\n",
    "  - pros\n",
    "    - avoids matching the token `red` in strings like `prediction`, i.e. avoids spurious matches\n",
    "  - cons\n",
    "    - tags are not all space separated so will not match anything, i.e. cannot find `white` in `whitesandalsbeach` \n",
    "- simple matching\n",
    "  - pros and cons are literally flip of that in exact matching, hard to say which is better\n",
    "  \n",
    "3. \"product descriptor\" is a merge of `title`, `title_orig`, `tags` and `product_color`\n",
    "- all seem useful\n",
    "- `title` and `title_orig` should be similar, one is French the other is English translation, who knows which is better, i.e. use both\n",
    "- `tags` seem very useful but format isn't standardized, some are space separated, others `;` separated, others NOT separated\n",
    "- `product_color` marginally useful\n",
    "\n",
    "4. given n tokens extracted from the `search_string`, trying to find matches for each token\n",
    "- can rank products on the number of `n` token hits\n",
    "- can also do a weighted token hit (based on the rarity of a token), i.e. rare tokens are stronger hits than common tokens, \"goth\" vs \"summer\"\n",
    "\n",
    "5. search tokens that receive 0 hits on the product list are useless, can give hints to the user to try different search strings\n",
    "\n",
    "6. top n text hits can be return along with their pictures as well for debugging / logic checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba59712",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a17e201c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ipynb_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1966c595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run class_Dataset.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe30f19",
   "metadata": {},
   "source": [
    "# IMPLEMENT TEXT SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "135010e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68678f",
   "metadata": {},
   "source": [
    "# FUNCTIONS TO BUILD UP QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fdb792c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLNAME            = str\n",
    "PRODUCT_DESCRIPTOR = pd.Series(dtype='str')\n",
    "SEARCH_STRING      = str\n",
    "TOKENS             = str\n",
    "SEARCH_TOKENS      = List[str]\n",
    "\n",
    "class TokenSearch():\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset : Dataset,\n",
    "        ) -> None :\n",
    "        self.dataset            = dataset\n",
    "        self.product_descriptor = self.make_product_descriptor()\n",
    "        \n",
    "    def make_product_descriptor(\n",
    "        self,\n",
    "        descriptor_colnames : List[COLNAME] = ['title','title_orig','tags','product_color'] # cols of interest\n",
    "        ) -> PRODUCT_DESCRIPTOR :\n",
    "        '''\n",
    "        TokenSearch.make_product_descriptor()\n",
    "        '''\n",
    "        descriptors = self.dataset.df[descriptor_colnames] # extract columns\n",
    "        descriptors = descriptors.apply(lambda x:' '.join([str(x) for x in x.values]),axis=1) # merge with | as sep\n",
    "        #descriptors = descriptors.str.replace('\\'','') # strip appostrophes\n",
    "        descriptors = descriptors.str.replace('\\W',' ',regex=True) # strip special chars\n",
    "        return descriptors\n",
    "\n",
    "    # search string tokenizer\n",
    "    def tokenize_search_string(\n",
    "        self,\n",
    "        search_string : SEARCH_STRING,\n",
    "        ) -> SEARCH_TOKENS :\n",
    "        tokens = re.split('\\W+',search_string) # partition string on non-alphanumeric chars\n",
    "        tokens = [token.lower() for token in tokens] # set tokens to lower case\n",
    "        tokens = list(set(tokens)) # uniques\n",
    "        return tokens\n",
    "\n",
    "    # token exact search augmenter for later regex usage\n",
    "    def exactify_search_tokens(\n",
    "        self,\n",
    "        search_tokens : SEARCH_TOKENS,\n",
    "        ) -> SEARCH_TOKENS :\n",
    "        '''\n",
    "        display(TokenSearch.tokenize_search_string('absc#dd\tddd  ,#,      asd;f    asdf asdf asdf asdf asdf asdfsd000_22220'))\n",
    "        display(TokenSearch.tokenize_search_string('womens banana    dress dress me me me'))\n",
    "        display(TokenSearch.exactify_search_tokens(tokenize_search_string('womens banana    dress dress me me me')))\n",
    "        '''\n",
    "        return ['\\\\b'+token+'\\\\b' for token in search_tokens] # force exact match, dont want to match 'red' from 'altered'\n",
    "    #bool(re.search('\\\\bsets\\\\b',make_product_descriptor()[0],flags=re.IGNORECASE))\n",
    "\n",
    "    # looking at all string product identifiers (title, title_orig, tags, product_color)\n",
    "    # count how many tokens found in each product_descriptor + weighted token search (e.g. 3x 2x 1x)\n",
    "    def tokens_found_count(\n",
    "        self,\n",
    "        #product_descriptor : pd.Series(dtype='str'),\n",
    "        search_tokens      : List[TOKENS]  = None,\n",
    "        search_string      : SEARCH_STRING = None,\n",
    "        exact_search       : bool          = False,\n",
    "        plot_top_n         : int           = None,\n",
    "        verbose            : int           = 0, # show workings\n",
    "        ) -> pd.Series(dtype='int') :\n",
    "        '''\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['summer','short'],verbose=1) # basic logic\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['harajuku','goth','sexy'],verbose=1) # test rare token logic\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['kids','top','banana','bobby','henry'],verbose=0) # test bad search tokens\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['kids','sandals'],verbose=1) # test\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['balloon'],plot_top_n=5,verbose=1) # test plot_top_n\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['balloon'],plot_top_n=5,verbose=1) # test results snipping\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['banana'],plot_top_n=5,verbose=1) # test results chat#\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['sandal'],plot_top_n=5,verbose=1) # test results chat\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['sandal','red'],plot_top_n=5,verbose=1) # test results chat\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_string='sandal red',plot_top_n=5,verbose=1) # test results chat\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['top','skinny','red'],plot_top_n=5,verbose=1) # test results chat\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_tokens=['top','skinny','red'],plot_top_n=5,exact_search=True,verbose=1) # test results chat\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_string='top red',plot_top_n=5,verbose=1) # test results chat\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_string='top red',plot_top_n=5,exact_search=True,verbose=1) # test exact search\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_string='exy',plot_top_n=5,verbose=1) # test exact search\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_string='exy',plot_top_n=5,exact_search=True,verbose=1) # test exact search\n",
    "        #tokens_found_count(product_descriptor=make_product_descriptor(),search_string='top red',plot_top_n=5,exact_search=True,verbose=1) # test exact search\n",
    "        '''\n",
    "        ###########################################################\n",
    "        # get search tokens\n",
    "        ###########################################################\n",
    "        if search_tokens is None:\n",
    "            search_tokens = self.tokenize_search_string(search_string=search_string) # strip them from search_string if search_tokens NOT provided\n",
    "        if verbose>1: print(f'search_tokens = {search_tokens}')\n",
    "            \n",
    "        ###########################################################\n",
    "        # amend tokens to exactify search on request\n",
    "        ###########################################################\n",
    "        if exact_search:\n",
    "            search_tokens_orig = search_tokens\n",
    "            search_tokens      = self.exactify_search_tokens(search_tokens)\n",
    "        if verbose>1: print(f'search_tokens = {search_tokens}')\n",
    "\n",
    "        ###########################################################\n",
    "        # figure out which tokens are found in product_descriptor\n",
    "        ###########################################################\n",
    "        # returns a List[pd.Series(dtype=bool)], each list entry denoteing if nth token is found (exact or simple matching)\n",
    "        found_count_list = [\n",
    "            self.product_descriptor.apply(\n",
    "                lambda x:bool(re.search(token,x,flags=re.IGNORECASE))\n",
    "            ) for token in search_tokens\n",
    "        ]\n",
    "        if verbose>1: print(f'found_count_list = {[x.sum() for x in found_count_list]}')\n",
    "\n",
    "        # df of Trues/Falses\n",
    "        found_count_df = pd.concat(found_count_list,axis=1)\n",
    "        found_count_df.columns = search_tokens\n",
    "        if verbose>1: print(f'found_count_df = {found_count_df}')\n",
    "\n",
    "        # series of found_count_series\n",
    "        found_count_series = found_count_df.sum(axis=1)\n",
    "        found_count_series.name = 'tokens_found_count'\n",
    "        if verbose>1: print(f'found_count_series = {found_count_series}')\n",
    "\n",
    "        ###########################################################\n",
    "        # token rarity\n",
    "        ###########################################################\n",
    "        # count rarity of token - rare tokens should be valued more\n",
    "        token_found_count           = found_count_df.sum()\n",
    "        if verbose>1: print('token_found_count');display(token_found_count)\n",
    "        token_rarity                = 1 - token_found_count / len(found_count_df) # rare tokens are valued more, only 1 instance (value ~= 1), 50% of products (value = 50%), 100% of products (value ~= 0)\n",
    "        if verbose>0: print('token_rarity');display(token_rarity)\n",
    "        discounted_token_df         = found_count_df * token_rarity # impact token found bool with value of token (between 0 and 1)\n",
    "        #return found_count_df,token_rarity\n",
    "        #print(found_count_df.shape)\n",
    "        #print(token_rarity.shape)\n",
    "        if verbose>2: print(discounted_token_df)\n",
    "        if verbose>2: print(discounted_token_df.max())\n",
    "        #display(discounted_token_df)\n",
    "        discounted_token_series     = discounted_token_df.sum(axis=1)\n",
    "        #display(discounted_token_series)\n",
    "        discounted_token_df.columns = ['discounted('+x+')' for x in discounted_token_df.columns] # rename df columns so result can exist in same df\n",
    "\n",
    "        ###########################################################\n",
    "        # give feedback if token not found\n",
    "        ###########################################################\n",
    "        unmatched_tokens = token_found_count[token_found_count==0]\n",
    "        if verbose>1: print(f'unmatched_tokens = {unmatched_tokens}')\n",
    "\n",
    "        # only give feedback if there are useless tokens\n",
    "        if len(unmatched_tokens)>0:\n",
    "            useless_tokens = ['\\'' + s + '\\'' for s in unmatched_tokens.index]\n",
    "            if verbose>1: display(useless_tokens)\n",
    "            if len(useless_tokens)==1:\n",
    "                helpful_string = useless_tokens[0]\n",
    "            else:\n",
    "                helpful_string = ', '.join(useless_tokens[:-1]) + ' and ' + useless_tokens[-1]\n",
    "\n",
    "            # show helping string\n",
    "            if exact_search:\n",
    "                helpful_string = helpful_string.replace('\\\\b','')\n",
    "                print(f'searchbot: no exact matches for {helpful_string}, try searching for something else')\n",
    "            else:\n",
    "                print(f'searchbot: no matches for {helpful_string}, try searching for something else')\n",
    "            print()\n",
    "\n",
    "        ###########################################################\n",
    "        # make pretty return df\n",
    "        ###########################################################\n",
    "        # [ found count ] + [ which tokens found ] + [search string]\n",
    "        #display(discounted_token_df.columns)\n",
    "        #display(found_count_df.columns)\n",
    "        found_count_summary = pd.concat(\n",
    "            [\n",
    "                found_count_series.to_frame('tokens_matched'),\n",
    "                found_count_df,\n",
    "                discounted_token_series.to_frame('discounted(tokens_matched)'),\n",
    "                discounted_token_df,\n",
    "                self.product_descriptor.to_frame('product_descriptor')\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # snip results table to stuff which has at least SOME match\n",
    "        found_count_summary = found_count_summary[found_count_summary['tokens_matched']>0]\n",
    "        # sort on some metric\n",
    "        found_count_summary = found_count_summary.sort_values(['tokens_matched']+['discounted(tokens_matched)'],ascending=False)\n",
    "\n",
    "        ###########################################################\n",
    "        # results chat\n",
    "        ###########################################################\n",
    "        if len(found_count_summary)==0:\n",
    "            print(f'results: I got nothing! T⌓T') # unhappy\n",
    "        elif len(found_count_summary)==1:\n",
    "            print(f'results: only 1 hit ￣ω￣, I hope it\\'s what you wanted!') # unsure\n",
    "        elif len(found_count_summary)<=5:\n",
    "            print(f'results: I only got {len(found_count_summary)} results, see anything you like?') # tight search\n",
    "        elif len(found_count_summary)<=10:\n",
    "            print(f'results: {len(found_count_summary)} results found') # normal\n",
    "        elif len(found_count_summary)>10:\n",
    "            print(f'results: {len(found_count_summary)} items found, I can do better if you can be more specific') # should do better\n",
    "\n",
    "        '''\n",
    "        # tag on searched string as well for debug purposes\n",
    "        if verbose>0:\n",
    "            found_count_summary = found_count_summary.join(product_descriptor.to_frame())\n",
    "        ''' \n",
    "\n",
    "        ###########################################################\n",
    "        # plot_top_n results\n",
    "        ###########################################################\n",
    "        if (plot_top_n is not None) and len(found_count_summary)>0:\n",
    "            self.dataset.get_product_pictures(locs=found_count_summary.index[:plot_top_n])\n",
    "            plt.show()\n",
    "\n",
    "        ###########################################################\n",
    "        # return\n",
    "        ###########################################################\n",
    "        if verbose>0:\n",
    "            return found_count_summary.head(20)\n",
    "        else:\n",
    "            return found_count_summary.head(plot_top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "828d01d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ts=TokenSearch(dataset=Dataset())\n",
    "#ts.tokens_found_count(search_tokens=['summer','short'],verbose=1) # basic logic\n",
    "#ts.tokens_found_count(search_tokens=['harajuku','goth','sexy'],verbose=1) # test rare token logic\n",
    "#ts.tokens_found_count(search_tokens=['kids','top','banana','bobby','henry'],verbose=0) # test bad search tokens\n",
    "#ts.tokens_found_count(search_tokens=['kids','sandals'],verbose=1) # test\n",
    "#ts.tokens_found_count(search_tokens=['balloon'],plot_top_n=5,verbose=1) # test plot_top_n\n",
    "#ts.tokens_found_count(search_tokens=['balloon'],plot_top_n=5,verbose=1) # test results snipping\n",
    "#ts.tokens_found_count(search_tokens=['banana'],plot_top_n=5,verbose=1) # test results chat#\n",
    "#ts.tokens_found_count(search_tokens=['sandal'],plot_top_n=5,verbose=1) # test results chat\n",
    "#ts.tokens_found_count(search_tokens=['sandal','red'],plot_top_n=5,verbose=1) # test results chat\n",
    "#ts.tokens_found_count(search_string='sandal red',plot_top_n=5,verbose=1) # test results chat\n",
    "#ts.tokens_found_count(search_tokens=['top','skinny','red'],plot_top_n=5,verbose=1) # test results chat\n",
    "#ts.tokens_found_count(search_tokens=['top','skinny','red'],plot_top_n=5,exact_search=True,verbose=1) # test results chat\n",
    "#ts.tokens_found_count(search_string='top red',plot_top_n=5,verbose=1) # test results chat\n",
    "#ts.tokens_found_count(search_string='top red',plot_top_n=5,exact_search=True,verbose=1) # test exact search\n",
    "#ts.tokens_found_count(search_string='exy',plot_top_n=5,verbose=1) # test exact search\n",
    "#ts.tokens_found_count(search_string='exy',plot_top_n=5,exact_search=True,verbose=1) # test exact search\n",
    "#ts.tokens_found_count(search_string='top red',plot_top_n=5,exact_search=True,verbose=1) # test exact search\n",
    "#found_count_df,token_rarity=\n",
    "#ts.tokens_found_count(search_string='hot banana skirt',plot_top_n=5,exact_search=True,verbose=1) # test exact search\n",
    "#ts.tokens_found_count(search_string='skirt slim',plot_top_n=5,exact_search=False,verbose=0) # test exact search\n",
    "ts.tokens_found_count(search_string='skirt slim',exact_search=False,verbose=0) # test exact search\n",
    "'''\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
