{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4828287d",
   "metadata": {},
   "source": [
    "# [TLDR] grep tokens in each products `product_descriptor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2073bf9e",
   "metadata": {},
   "source": [
    "# [LONGER VERSION]\n",
    "- token matching is most direct way to grab what the customer is looking for\n",
    "\n",
    "- upon being given a `search_string`, simple regex magic is done\n",
    "  - to partition the `search_string` into `token`s\n",
    "  - I then grep these `token` in each products `product_descriptor`, matching can then be either\n",
    "    - exact matching (must have leading and trailing spaces) or \n",
    "    - simply on presence (presence is enough, this works well for poorly partitioned `tags`)\n",
    "  - matching can also be\n",
    "    - case sensitive or\n",
    "    - case insenstive\n",
    "  \n",
    "- `product_descriptor` is built by combining from all string like info deemed to be useful, namely\n",
    "    - `title` (~ French description)\n",
    "    - `title_orig` (~ English translation though as the translation could be wrong there is no harm in doubling up)\n",
    "    - `tags` (~ non-standard category labelling of the product)\n",
    "      - is sometimes partitioned with space and ;\n",
    "      - and others appears as a continuous string (non-exact matching is better for this)\n",
    "    - `product_color` (self explanatory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba59712",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a17e201c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ipynb_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1966c595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run class_Dataset.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe30f19",
   "metadata": {},
   "source": [
    "# IMPLEMENT TEXT SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "135010e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68678f",
   "metadata": {},
   "source": [
    "# FUNCTIONS TO BUILD UP QUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdb792c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGEX_PATTERN      = str\n",
    "COLNAME            = str\n",
    "PRODUCT_DESCRIPTOR = pd.Series(dtype='str')\n",
    "SEARCH_STRING      = str\n",
    "TOKENS             = str\n",
    "SEARCH_TOKENS      = List[str]\n",
    "\n",
    "class TokenSearch():\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset : Dataset,\n",
    "        ) -> None :\n",
    "        self.dataset            = dataset\n",
    "        self.product_descriptor = self.make_product_descriptor()\n",
    "        \n",
    "    def make_product_descriptor(\n",
    "        self,\n",
    "        descriptor_colnames : List[COLNAME] = ['title','title_orig','tags','product_color'] # cols of interest\n",
    "        ) -> PRODUCT_DESCRIPTOR :\n",
    "        '''\n",
    "        TokenSearch.make_product_descriptor()\n",
    "        '''\n",
    "        descriptors = self.dataset.df[descriptor_colnames] # extract columns\n",
    "        descriptors = descriptors.apply(lambda x:' '.join([str(x) for x in x.values]),axis=1) # merge with | as sep\n",
    "        #descriptors = descriptors.str.replace('\\'','') # strip appostrophes\n",
    "        descriptors = descriptors.str.replace('\\W',' ',regex=True) # strip special chars\n",
    "        return descriptors\n",
    "\n",
    "    # search string tokenizer\n",
    "    def tokenize_search_string(\n",
    "        self,\n",
    "        search_string : SEARCH_STRING,\n",
    "        exact_match   : bool = False,\n",
    "        ) -> SEARCH_TOKENS :\n",
    "        tokens = re.split('\\W+',search_string) # partition string on non-alphanumeric chars\n",
    "        #tokens = [token.lower() for token in tokens] # set tokens to lower case\n",
    "        tokens = list(set(tokens)) # uniques\n",
    "        if exact_match: return ['\\\\b'+token+'\\\\b' for token in tokens] # make tokens exact matches\n",
    "        return tokens\n",
    "\n",
    "    def build_regex_from_tokens(\n",
    "        self,\n",
    "        search_tokens : SEARCH_TOKENS,\n",
    "        ) -> REGEX_PATTERN :\n",
    "        base = r'^{}'\n",
    "        expr = '(?=.*{})'\n",
    "        return base.format(''.join(expr.format(w) for w in search_tokens))\n",
    "    #display(build_regex(ts.tokenize_search_string('RED banana')))\n",
    "    \n",
    "    def tokens_found(\n",
    "        self,\n",
    "        search_tokens      : List[TOKENS]  = None, # can give either search_string or search_tokens\n",
    "        search_string      : SEARCH_STRING = None, # can give either search_string or search_tokens\n",
    "        exact_match        : bool          = False,\n",
    "        case_sensitive     : bool          = False,\n",
    "        verbose            : int           = 0, # show workings\n",
    "        ) -> pd.Series(dtype='int') :\n",
    "        ###########################################################\n",
    "        # build regex for pd.Series.str.contains()\n",
    "        ###########################################################\n",
    "        # get search tokens if not already given\n",
    "        if search_tokens is None:\n",
    "            search_tokens = self.tokenize_search_string(\n",
    "                search_string = search_string,\n",
    "                exact_match   = exact_match,\n",
    "            )\n",
    "        if verbose>1: print(f'search_tokens = {search_tokens}')\n",
    "\n",
    "        # compile regex\n",
    "        regex_str = self.build_regex_from_tokens(search_tokens)\n",
    "        if verbose>1: print(f'regex_str = {regex_str}')\n",
    "        \n",
    "        ###########################################################\n",
    "        # apply regex to product_descriptor\n",
    "        ###########################################################\n",
    "        # returns a List[pd.Series(dtype=bool)], each list entry denoteing if nth token is found (exact or simple matching)\n",
    "        if verbose>1: print(self.product_descriptor.shape)\n",
    "        if case_sensitive:\n",
    "            found_series = self.product_descriptor.str.contains(regex_str)\n",
    "        else:\n",
    "            found_series = self.product_descriptor.str.contains(regex_str,flags=re.IGNORECASE)\n",
    "        \n",
    "        if verbose>1: print(found_series.shape)\n",
    "        \n",
    "        ###########################################################\n",
    "        # make pretty return df\n",
    "        ###########################################################\n",
    "        if verbose > 0:\n",
    "            # snip results table to stuff which has at least SOME match\n",
    "            filter_me = pd.concat(\n",
    "                [\n",
    "                    self.product_descriptor.to_frame('product_descriptor'),\n",
    "                    self.dataset.df, #found_series.to_frame('found'),\n",
    "                ],\n",
    "                axis=1\n",
    "            )\n",
    "        else:\n",
    "            filter_me = self.dataset.df\n",
    "        \n",
    "        # apply filter\n",
    "        res = filter_me[found_series]\n",
    "\n",
    "        ###########################################################\n",
    "        # return\n",
    "        ###########################################################\n",
    "        if verbose>0:\n",
    "            return res.head(20)\n",
    "        else:\n",
    "            return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
