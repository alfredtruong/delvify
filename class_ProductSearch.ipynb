{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba59712",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ipynb_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfba2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run class_Dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run class_TokenSearch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7428c",
   "metadata": {},
   "source": [
    "# CLASS DEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c907a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductSearch():\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ) -> None :\n",
    "\n",
    "        self.dataset       = Dataset() # initialize Wish dataset\n",
    "        self.token_search  = TokenSearch(dataset=self.dataset) # prep token searcher\n",
    "        self.nearby_search = NeighbourSearch(dataset=self.dataset) # prep nearest neighbor searcher\n",
    "\n",
    "    # this given top rating products precedence over units sold, allows good products to trend rather than old products to stay at the top\n",
    "    def reorder_on_top_ratings(\n",
    "        self,\n",
    "        res : pd.DataFrame,\n",
    "        ) -> pd.DataFrame :\n",
    "        top_rating_pctage = (res['rating_five_count']+res['rating_four_count'])/res['rating_count']\n",
    "        top_rating_pctage = top_rating_pctage.sort_values(ascending=False)\n",
    "        return res.loc[top_rating_pctage.index,:]\n",
    " \n",
    "    # find products that match purely on tokens + sort them on \"top ratings\" (allow new top rated products to trend rather than old products with any units_sold to dominate results)\n",
    "    def token_matching_results(\n",
    "        self,\n",
    "        search_string  : str,\n",
    "        exact_match    : bool = False,\n",
    "        case_sensitive : bool = False,\n",
    "        verbose        : int  = 0\n",
    "        ) -> pd.DataFrame :\n",
    "        # step 1 = apply text search to match tokens in search string to that of the product descriptor\n",
    "        token_search_results = self.token_search.tokens_found_count(\n",
    "            search_string  = search_string,\n",
    "            exact_match    = exact_match,\n",
    "            case_sensitive = case_sensitive,\n",
    "            verbose        = verbose,\n",
    "        )\n",
    "    \n",
    "        # step 2 = reorder results on \"top ratings\"\n",
    "        token_search_results = self.reorder_on_top_ratings(token_search_results)\n",
    "        \n",
    "        # return\n",
    "        return token_search_results\n",
    "    \n",
    "    # grab incrementally more nearest neighbours from some `source_results` list until we get more than we need\n",
    "    def nearby_results(\n",
    "        self,\n",
    "        source_results         : pd.DataFrame,\n",
    "        total_required_results : int,\n",
    "        ):\n",
    "        # initialize while loop\n",
    "        res       = source_results.index\n",
    "        n_nearest = 1\n",
    "        \n",
    "        # look outwards and find nearest results (including self) until we get more that what we need\n",
    "        while len(res) < total_required_results:\n",
    "            # get next nearest result from each source_result\n",
    "            res = self.nearby_search.get_n_nearest_from_locs(\n",
    "                n_nearest = n_nearest,\n",
    "                locs      = source_results,\n",
    "            )\n",
    "            \n",
    "            n_nearest = n_nearest + 1\n",
    "        \n",
    "        # have more than `actual_required_results`\n",
    "        return res\n",
    "\n",
    "        ########################################\n",
    "        # figure out how many more results we need\n",
    "        # - have len(`source_results`)\n",
    "        # - need `n_results_needed` more\n",
    "        ########################################\n",
    "        \n",
    "    # find products that match purely on tokens + sort them on \"top ratings\" (allow new top rated products to trend rather than old products with any units_sold to dominate results)\n",
    "    def query(\n",
    "        self,\n",
    "        search_string       : str,\n",
    "        exact_match         : bool  = False,\n",
    "        case_sensitive      : bool  = False,\n",
    "        required_results    : int   = 20, # token matching results at the top (with )\n",
    "        extra_results_ratio : float = 1.5 # ratio vs `required_results` of additional results needed for image search to to help refine / acquire\n",
    "        show_top_n          : int   = 5,\n",
    "        chatty              : bool  = True,\n",
    "        ) -> pd.DataFrame :\n",
    "        ###########################################################\n",
    "        # step 1 = get token_matching_results\n",
    "        ###########################################################\n",
    "        token_results = self.token_matching_results(\n",
    "            search_string  = search_string,\n",
    "            exact_match    = exact_match,\n",
    "            case_sensitive = case_sensitive,\n",
    "            verbose        = verbose,\n",
    "        )\n",
    "        \n",
    "        ###########################################################\n",
    "        # step 2 = if token_matching_results not enough, grab \"nearby products\" and use ImageSearch to help weed through / refine\n",
    "        ###########################################################\n",
    "        if len(token_results) < required_results:\n",
    "            if requested_total_results < 1:\n",
    "                raise ValueError('required_total_results needs to be >= 1')\n",
    "                \n",
    "            # get more results with nearest neighbour\n",
    "            res = self.nearby_results(\n",
    "                source_results         = token_results, # use token matching results as source to search from\n",
    "                total_required_results = int(required_results * extra_results_ratio),\n",
    "            )\n",
    "            \n",
    "            # get unique nearby results\n",
    "            unique_nearby_results = res[res!=0] # ignore home truths\n",
    "            unique_nearby_results = unique_nearby_results.index.unique() # throw away duplicates if they exist\n",
    "            \n",
    "            # order results on image similarity\n",
    "            ###########################################################\n",
    "            # step 2 = if token_matching_results not enough, grab \"nearby products\" and use ImageSearch to help weed through / refine\n",
    "            ###########################################################\n",
    "    \n",
    "        \n",
    "        ###########################################################\n",
    "        # plot_top_n results\n",
    "        ###########################################################\n",
    "        self.dataset.show_top_n(token_search_results,n=show_top_n)\n",
    "\n",
    "        ###########################################################\n",
    "        # results chat\n",
    "        ###########################################################\n",
    "        res = token_search_results\n",
    "        if chatty:\n",
    "            if len(res)==0:    print(f'results: I got nothing! T⌓T') # unhappy\n",
    "            elif len(res)==1:  print(f'results: only 1 hit ￣ω￣, I hope it\\'s what you wanted!') # unsure\n",
    "            elif len(res)<=5:  print(f'results: {len(res)} results, see anything you like?') # tight search\n",
    "            elif len(res)<=10: print(f'results: {len(res)} results found') # normal\n",
    "\n",
    "        return token_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd000565",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pd.Series([1,2,3,4,5],index=[1,1,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e665189",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad7381e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ps=ProductSearch()\n",
    "res=ps.query('harajuku')\n",
    "#ps.query('harajuku beach')\n",
    "#ps.query('harajuku pop')\n",
    "#ps.query('dress')\n",
    "#ps.query('dress beach')\n",
    "#ps.query('dress beach flower')\n",
    "#ps.query('dress beach flower sleeve')\n",
    "#ps.query('dress beach flower blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c4f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.reorder_on_top_ratings(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991b4a00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res.loc[:,['units_sold','rating']+[x for x in res.columns if x not in ['units_sold','rating']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef87323",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (res['rating_five_count']+res['rating_four_count'])/res['rating_count']\n",
    ").sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23097be",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[['units_sold','rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c7f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(lambda x:(x['rating_five_count']+x['rating_four_count'])/x['rating_count'])(ps.dataset.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926bc805",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res.sort_values(['units_sold','rating'],ascending=[False,False])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
